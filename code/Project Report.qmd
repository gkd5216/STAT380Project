---
title: "Project Report"
subtitle: "STAT 380 Section 004"
author: "Gianna DeLorenzo & Melissa Kim"
format:
  pdf:
    toc: FALSE
    number-sections: TRUE
    number-depth: 5
    fig-align: center
    cap-location: top
    geometry:
      - top=1in
      - left=1in
      - right=1in
      - bottom=1in
    colorlinks: TRUE
execute:
  echo: FALSE
  warning: FALSE
  cache: TRUE
  message: FALSE

bibliography: references.bib  
csl: apa7.csl  
---

# Title:

## Introduction

Patient experiences with medications is crucial when it comes to the healthcare domain by providing insights on the side effects, drug effectiveness, and overall satisfaction.

**Research Question**

In this report, we will be exploring the following research question:

**Can machine learning tasks predict the overall satisfaction of the patients with a particular class of drugs for a given medical condition?**

This research question

## Exploratory Data Analysis

### Variable Description

The response variable of interest is the ratings of the drugs with the negative reviews from **0-6** and the positive reviews from **7-10**.

### Data Visualization

```{r echo=FALSE, message=FALSE}
#| label: Table1
#| fig-cap: "Table 1"
#| fig-pos: H
#| fig-height: 5
#| fig-width: 7
#| fig-alt: "Table showing Variables"
#| aria-describedby: namesPlotLD
#| lst-label: lst-figure2
#| lst-cap: "Code Chunk for Making Figure 1"

#Load necessary libraries
library(dplyr) #data manipulation
library(knitr) #table formats
library(readr) 
library(kableExtra)
library(caret)
library(forcats)
library(nnet)

#Import UC Irvine test csv file. (Initial Definition for All Code)
drug_train <- read_tsv(url("https://raw.githubusercontent.com/gkd5216/STAT380Project/refs/heads/main/data/drugLibTest_raw.tsv"))
drug_test <- read_tsv(url("https://raw.githubusercontent.com/gkd5216/STAT380Project/refs/heads/main/data/drugLibTrain_raw.tsv"))

# Summary Statistics grouping by Geographic Area.
variable_analysis <- data.frame(
  Variable = c("reviewID", "urlDrugName", "rating", "effectiveness", "sideEffects", "condition", "benefitsReview", 
               "sideEffectsReview", "commentsReview"),
  Type = c("Integer", "Categorial", "Integer", "Categorical", "Categorical", "Categorical", 
           "Categorical", "Categorical", "Categorical"),
  Explanation = c(
    "Review ID",
    "Name of drug",
    "Name of condition",
    "Patient on benefits",
    "Patient on side effects",
    "Overall patient comment",
    "10 star patient rating",
    "5 step side effect rating",
    "5 step effectiveness rating"
  )
)
  
# Outputs Formatted Summary Table with Kable Styling tools
kable(
  variable_analysis,
  caption = "Variables used in Analysis"
  )

```

### Data Cleaning

This glimpse of the Drug Reviews train data displays a dataset in need of being cleaned and tidied.

```{r, message=FALSE}
library(tidyverse)
library(janitor)
glimpse(drug_train)

```

```{r}
tidy_drug_train <- drug_train %>%
  rename(id = `...1`) %>% #Renames ID column
  clean_names() %>%
  mutate(
    rating = as.numeric(rating),                # Ensure numeric
    effectiveness = as.factor(effectiveness),   # Treat as ordinal later if needed
    side_effects = as.factor(side_effects),
    condition = str_to_title(condition),        # Title case for condition
    drug_name = str_to_title(url_drug_name),    # Clean drug name
    full_review = paste(benefits_review, side_effects_review, comments_review, sep = " ") %>%
      str_squish()                              # Collapse and trim whitespace
  ) %>%
  select(id, drug_name, condition, rating, effectiveness, side_effects, full_review) %>% 
  filter(!is.na(rating), !is.na(condition), condition != "")

tidy_drug_train
```

## Modeling

### Logistic Regression: Predicting Positive vs. Negative Reviews

Can we predict if a patient’s review is positive or negative based on their feedback?

```{r}
# Create binary sentiment variable: 1 (positive w/ rating 7–10), 0 (negative w/ rating 1–6)

drug_train <- drug_train %>%
  mutate(sentiment = ifelse(rating >= 7, 1, 0))

drug_test <- drug_test %>%
  mutate(sentiment = ifelse(rating >= 7, 1, 0))

drug_train <- drug_train %>%
  mutate(across(c(urlDrugName, condition, effectiveness), as.factor))

drug_test <- drug_test %>%
  mutate(across(c(urlDrugName, condition, effectiveness), as.factor))

drug_train <- drug_train %>%
  mutate(urlDrugName = fct_lump(urlDrugName, n = 20),
         condition = fct_lump(condition, n = 20),
         effectiveness = fct_lump(effectiveness, n = 5))

# Match levels in test data to training data
drug_test <- drug_test %>%
  mutate(urlDrugName = fct_lump(urlDrugName, n = 20),
         condition = fct_lump(condition, n = 20),
         effectiveness = fct_lump(effectiveness, n = 5))

# Align factor levels
drug_test$urlDrugName <- factor(drug_test$urlDrugName, levels = levels(drug_train$urlDrugName))

drug_test$condition <- factor(drug_test$condition, levels = levels(drug_train$condition))
drug_test$effectiveness <- factor(drug_test$effectiveness, levels = levels(drug_train$effectiveness))

# Remove rows with NAs
test_data_clean <- drug_test %>%
  filter(!is.na(urlDrugName) & !is.na(condition) & !is.na(effectiveness))

# Logistic regression model
logit_model <- glm(sentiment ~ urlDrugName + condition + effectiveness,
                   data = drug_train, family = "binomial")

# Predict on clean test data
test_pred <- predict(logit_model, newdata = test_data_clean, type = "response")
test_pred_class <- ifelse(test_pred > 0.5, 1, 0)

# Confusion matrix
confusionMatrix(factor(test_pred_class), factor(test_data_clean$sentiment))

```

Confusion Matrix 0 = negative review, 1 = positive review

True Negatives (TN) = 212 212 reviews were predicted negative and actually negative

False Positives (FP) = 107 107 reviews were predicted positive but actually negative

False Negatives (FN) = 48 48 reviews were predicted negative but actually positive

True Positives (TP) = 537 537 reviews were predicted positive and actually positive

| Metric | Value | Meaning |
|---------------|------------|-----------------------------|
| Accuracy | 0.8285 | 82.85% of predictions were correct overall |
| Sensitivity (Class 0) | 0.6646 | 66.46% of actual negative reviews were correctly identified |
| Specificity (Class 1) | 0.9179 | 91.79% of actual positive reviews were correctly identified |
| Kappa | 0.6081 | Moderate to strong agreement beyond chance, follows within the “good” range of 0.6-0.8 |
| Precision (class 0) | 0.8154 | Out of all predicted negative reviews, 81.54% were correct |
| Negative Predictive Value | 0.8339 | Out of all predicted positive reviews, 83.39% were correct |
| Balanced Accuracy | 0.7913 | Average of sensitivity and specificity; useful for imbalanced classes |
| No Information Rate (NIR) | 0.6471 | 64.71% of reviews in the test set are in the majority class (positive); always guessing the majority class (positive) would be correct 64.71% of the time |
| McNemar’s Test | p = 3.183e-06 | Suggests a significant difference in types of errors; model is not just guessing randomly |

***Model Strengths:***  - High accuracy (82.9%) - Excellent at identifying positive reviews (specificity = 91.8%) - High precision for negative class (precision = 81.5%), so false alarms are limited - Performs much better than guessing the majority class (accuracy \> NIR)

***Model Weaknesses:***  - Struggles more with detecting negative reviews (sensitivity = 66.5%), misses about 1 in 3 negative reviews

***Conclusion*** The logistic regression model achieved an overall accuracy of 82.85%, which significantly outperforms the no-information rate of 64.71% (p \< 2.2e-16). It performs well in identifying positive reviews (specificity = 91.8%) but is less sensitive to negative reviews (sensitivity = 66.5%). The balanced accuracy of 79.1% confirms that the model maintains reasonable performance across both classes. The confusion matrix and McNemar's test (p = 3.18e-06) further indicate that the model is not biased toward one class and captures meaningful patterns in patient review sentiment.

***Answer to “Can we predict if a patient’s review is positive or negative based on their feedback?”*** Yes, we can predict whether a patient’s review is positive or negative using machine learning. Our logistic regression model, trained on features such as the drug name, medical condition, and reported effectiveness, achieved an overall accuracy of 82.85%, which was much higher than the no-information rate of 64.71%. The model showed strong performance in identifying positive reviews (specificity = 91.8%) and reasonably good performance for negative reviews (sensitivity = 66.5%). These results indicate that the model can reliably classify overall patient sentiment based on the structured data.

### Neutral Network: Predicting Ratings with Nuance

Can we predict a more nuanced outcome of the review based on patient attributes, dosage, and drug features?

```{r}
# Drop NAs
drug_train <- drug_train %>%
  select(rating, urlDrugName, condition, effectiveness) %>%
  drop_na()

# Treat rating as factor
drug_train$rating <- as.factor(drug_train$rating)

drug_train <- drug_train %>%
  mutate(
    urlDrugName = fct_lump(urlDrugName, n = 20),
    condition = fct_lump(condition, n = 20),
    effectiveness = fct_lump(effectiveness, n = 5)
  )

# Split into training and validation sets
set.seed(42)
split_index <- createDataPartition(drug_train$rating, p = 0.8, list = FALSE)
train_set <- drug_train[split_index, ]
valid_set <- drug_train[-split_index, ]

# Train the neural network model
nn_model <- nnet(rating ~ urlDrugName + condition + effectiveness,
                 data = train_set,
                 size = 5,
                 maxit = 200,
                 trace = FALSE)

# Predict on validation set
nn_pred <- predict(nn_model, newdata = valid_set, type = "class")

nn_pred <- factor(nn_pred, levels = levels(valid_set$rating))
valid_rating <- factor(valid_set$rating, levels = levels(valid_set$rating))

# Evaluate model
confusionMatrix(nn_pred, valid_rating)

```

### KNN (K-Nearest Neighbors)

Can we classify the sentiment of a new review based on sentiments of similar past reviews?

### K-means Clustering

## Discussion

## References

# Code Appendix

```{r codeAppend, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
