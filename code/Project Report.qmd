---
title: "Project Report"
subtitle: "STAT 380 Section 004"
author: "Gianna DeLorenzo & Melissa Kim"
format:
  pdf:
    toc: FALSE
    number-sections: TRUE
    number-depth: 5
    fig-align: center
    cap-location: top
    geometry:
      - top=1in
      - left=1in
      - right=1in
      - bottom=1in
    colorlinks: TRUE
execute:
  echo: FALSE
  warning: FALSE
  cache: TRUE
  message: FALSE

bibliography: references.bib  
csl: apa7.csl  
---

# Patient Satisfaction Analysis with Drug Reviews

Patient experiences with medications is crucial when it comes to the healthcare domain. These drug reviews give valuable insights into aspects such as side effects, drug effectiveness, and overall patient satisfaction. Our project explores the ***Drug Reviews*** dataset from the UC Irvine Repository.

In this report, we will be exploring the following **research question**:

-   **Can machine learning tasks predict the overall satisfaction of the patients with a particular class of drugs for a given medical condition?**

This question explores the patterns with machine learning tasks and their effectiveness on predicting the satisfaction of patients based on a given class of drug.

We will first be presenting the background of the dataset and related work. Then,

## Background & Related Work

The **Drug Reviews** dataset provides a useful source of information beyond just the structured clinical trial data, offering insights into the real-world drug effectiveness, side effects, and the overall patient satisfaction. The Health and Medicine field usually leverages techniques from natural languages processing (NLP) and machine learning in order to get the information from patient's textual data and understand it. Prior research has explored the identification of adverse drug reactions using NLP and the analysis of patient sentiments regarding drug experiences.

The first piece of related work is the introductory paper for the Drug Reviews dataset is titled **Aspect-Based Sentiment Analysis of Drug Reviews Applying Cross-Domain and Cross-Data Learning** [^1] by F. Gräßer, S. Kallumadi, H. Malberg, and S. Zaunseder. This research paper demonstrated the applicability of sentiment analysis to drug reviews. This includes classifying the overall patient satisfaction and investigating the challenges with data limitations and transferring models across various domains and data sources.

[^1]: Gräßer, F., Kallumadi, S., Malberg, H., & Zaunseder, S. (2018). Aspect-Based Sentiment Analysis of Drug Reviews Applying Cross-Domain and Cross-Data Learning. Proceedings of the 2018 International Conference on Digital Health.

Furthermore, the application of machine learning with predicting patient sentiments/ratings and satisfaction from drug reviews is an active area. The second piece of related work is a research paper that was highly influencial to the production of the introductory paper. This paper is titled **Predicting Sentiments of Users About Medical Treatments using Pre-trained Large Language Models** [^2] by A. Hassan, J. Bagherzadeh, M. Khorasani, and A. Sorayaie Azar. This research paper explores the usage of various machine learning techniques including traditional models and large language models (LLMs). These models were used to predict the ratings on drug reviews and achieve high accuracies in classification tasks.

[^2]: Hassan, A., Mohasefi, J.B., & Azar, S. (2025). Predicting Sentiments of Users about Medical Treatments using Pre-trained Large Language Models. Journal of Information Systems Engineering and Management.

## Data Description & Preprocessing

### Provenance

The dataset used for this analysis, the ***Drug Reviews*** dataset from the UC Irvine Repository [^3], explores reviews from patients on specific drugs along with related medical conditions. The reviews and ratings are grouped into reports on the three aspects being the benefits, side effects and overall comment. The data was originally obtained by crawling online pharmaceutical review sites. This dataset is split into 2 tsv files with one being a train set (75%) and the other one being a test set (25%). It contains 4143 instances and 8 features.  

[^3]: Kallumadi, S. & Grer, F. (2018). Drug Reviews (Druglib.com) \[Dataset\]. UCI Machine Learning Repository. https://doi.org/10.24432/C55G6J.

**Source**: [Drug Effects Data](https://archive.ics.uci.edu/dataset/461/drug+review+dataset+druglib+com).

```{r echo=FALSE, message=FALSE}
#| label: Table0
#| fig-cap: "Dataset"
#| fig-pos: H
#| fig-height: 5
#| fig-width: 7
#| fig-alt: "Drug Reviews dataset"
#| aria-describedby: namesPlotLD
#| lst-label: lst-figure2
#| lst-cap: "Code Chunk for importing dataset"

#Load necessary libraries
library(dplyr) #data manipulation
library(knitr) #table formats
library(readr) 
library(kableExtra)
library(caret)
library(forcats)
library(nnet)
library(kknn)
library(tidyr)
library(broom)
library(ggcorrplot)

#Import UC Irvine test tsv files. (Initial Definition for All Code)
drug_train_tsv <- read_tsv("https://raw.githubusercontent.com/gkd5216/STAT380Project/refs/heads/main/data/drugLibTest_raw.tsv")
drug_test_tsv <- read_tsv("https://raw.githubusercontent.com/gkd5216/STAT380Project/refs/heads/main/data/drugLibTrain_raw.tsv")

```

### Variable Description

The response variable is the patient's overall ratings of the drugs with the negative reviews from **0-6** and the positive reviews from **7-10**. 

An example of a **high review's** (Rating = 10) textual review includes the following:

-   **urlDrugName**: Xanax
-   **Rating**: 10
-   **benefitsReview**: This simply just works fast and without any of the nasty side effects of SSRI medicines...
-   **sideEffectsReview**: I really don't have any side effects other than the mild but very tolerable issue of taking it 4-5 times a day which isn't an effect but just the way you need to take this...
-   **commentsReview**: I first started taking this at 3 times per day with .25 mg pills. I was advised not to take as needed with my panic attacks as that would reinforce the idea of taking a pill when it's needed and not help me work around my anxiety...

An example of a **low review's** (Rating = 1) textual review includes the following:

-   **urlDrugName**: Claritin

-   **Rating**: 1

-   **benefitsReview**: None - did nothing to help allergies. I just had a dryer, more painful version of my allergies with new allergies/irritations developing on top of my original ones...

-   **sideEffectsReview**: I had some horrifying mental and physical side effects. ruined a year of my life. read this - it mentions everything I dealt with: http://www.askapatient.com/viewrating.aspdrug=19658&name=CLARITIN

-   **commentsReview**: Took one 10 mg pill nightly...

### Data Cleaning and Preprocessing

Contrary to the UC Irvine Repository stating "No Missing Values" for the dataset, there were missing values observed in the text review field including benefitsReview, sideEffectsReview, and commentsReview. We initially used glimpse() to inspect the train and test datasets to possibly find missing values. It did not show any missing values in either dataset, but it did provide us with useful information about the variables such as the data types and structure. 

```{r echo=FALSE, message=FALSE}
glimpse(drug_train_tsv)
glimpse(drug_test_tsv)
```

Only the drug test dataset outputted with a missing value being present, so we figured out what column and row contains this. Then, we figured out that the commentsReview column contains 1 missing value, but it is not specifically an N/A column. It states "N/A currently," so it did not seem necessary to take care of this. 

```{r echo=FALSE, message=FALSE}
any(is.na(drug_train_tsv))
any(is.na(drug_test_tsv))
colSums(is.na(drug_test_tsv))
```

Beyond handling the missing values, preprocessing steps were tailored based on the needs of the various machine learning models. For models such as Logistic Regression and Neural Networks, the structured variables (urlDrugName, condition, effectiveness, sideEffects, rating) were utilized. 

```{r echo=FALSE, message=FALSE}
#| label: Table1
#| fig-cap: "Table 1"
#| fig-pos: H
#| fig-height: 5
#| fig-width: 7
#| fig-alt: "Table showing Variables"
#| aria-describedby: namesPlotLD
#| lst-label: lst-figure2
#| lst-cap: "Code Chunk for Making Figure 1"

# Summary Statistics grouping by Geographic Area.
variable_analysis <- data.frame(
  Variable = c("reviewID", "urlDrugName", "rating", "effectiveness", "sideEffects", "condition", "benefitsReview", 
               "sideEffectsReview", "commentsReview"),
  Type = c("Integer", "Categorial", "Integer", "Categorical", "Categorical", "Categorical", 
           "Categorical", "Categorical", "Categorical"),
  Explanation = c(
    "Review ID",
    "Name of drug",
    "Name of condition",
    "Patient on benefits",
    "Patient on side effects",
    "Overall patient comment",
    "10 star patient rating",
    "5 step side effect rating",
    "5 step effectiveness rating"
  )
)
  
# Outputs Formatted Summary Table with Kable Styling tools
kable(
  variable_analysis,
  caption = "Variables used in Analysis"
  )

```

### Exploratory Data Analysis

```{r}
ggplot(drug_train_tsv, aes(x = rating)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(title = "Distribution of Patient Ratings (Train)", x = "Rating (1-10)", y = "Number of Reviews")

ggplot(drug_test_tsv, aes(x = rating)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(title = "Distribution of Patient Ratings (Test)", x = "Rating (1-10)", y = "Number of Reviews")
```

## Methodology

### Data Wrangling

## Model Results

### Logistic Regression: Predicting Positive vs. Negative Reviews

Can we predict if a patient’s review is positive or negative based on their feedback?

```{r, message=FALSE, warning=FALSE}
# Create binary sentiment variable: 1 (positive w/ rating 7–10), 0 (negative w/ rating 1–6)

drug_train <- drug_train_tsv %>%
  mutate(sentiment = ifelse(rating >= 7, 1, 0))

drug_test <- drug_test_tsv %>%
  mutate(sentiment = ifelse(rating >= 7, 1, 0))

drug_train <- drug_train %>%
  mutate(across(c(urlDrugName, condition, effectiveness), as.factor))

drug_test <- drug_test %>%
  mutate(across(c(urlDrugName, condition, effectiveness), as.factor))

drug_train <- drug_train %>%
  mutate(urlDrugName = fct_lump(urlDrugName, n = 20),
         condition = fct_lump(condition, n = 20),
         effectiveness = fct_lump(effectiveness, n = 5))

# Match levels in test data to training data
drug_test <- drug_test %>%
  mutate(urlDrugName = fct_lump(urlDrugName, n = 20),
         condition = fct_lump(condition, n = 20),
         effectiveness = fct_lump(effectiveness, n = 5))

# Align factor levels
drug_test$urlDrugName <- factor(drug_test$urlDrugName, levels = levels(drug_train$urlDrugName))
drug_test$condition <- factor(drug_test$condition, levels = levels(drug_train$condition))
drug_test$effectiveness <- factor(drug_test$effectiveness, levels = levels(drug_train$effectiveness))

# Remove rows with NAs
test_data_clean <- drug_test %>%
  filter(!is.na(urlDrugName) & !is.na(condition) & !is.na(effectiveness))

# Logistic regression model
logit_model <- glm(sentiment ~ urlDrugName + condition + effectiveness,
                   data = drug_train, family = "binomial")

# Predict on clean test data
test_pred <- predict(logit_model, newdata = test_data_clean, type = "response")
test_pred_class <- ifelse(test_pred > 0.5, 1, 0)

# Confusion matrix
confusionMatrix(factor(test_pred_class), factor(test_data_clean$sentiment))

```

Confusion Matrix 0 = negative review, 1 = positive review

True Negatives (TN) = 212 212 reviews were predicted negative and actually negative

False Positives (FP) = 107 107 reviews were predicted positive but actually negative

False Negatives (FN) = 48 48 reviews were predicted negative but actually positive

True Positives (TP) = 537 537 reviews were predicted positive and actually positive

| Metric | Value | Meaning |
|--------------------|-------------------|---------------------------------|
| Accuracy | 0.8285 | 82.85% of predictions were correct overall |
| Sensitivity (Class 0) | 0.6646 | 66.46% of actual negative reviews were correctly identified |
| Specificity (Class 1) | 0.9179 | 91.79% of actual positive reviews were correctly identified |
| Kappa | 0.6081 | Moderate to strong agreement beyond chance, follows within the “good” range of 0.6-0.8 |
| Precision (class 0) | 0.8154 | Out of all predicted negative reviews, 81.54% were correct |
| Negative Predictive Value | 0.8339 | Out of all predicted positive reviews, 83.39% were correct |
| Balanced Accuracy | 0.7913 | Average of sensitivity and specificity; useful for imbalanced classes |
| No Information Rate (NIR) | 0.6471 | 64.71% of reviews in the test set are in the majority class (positive); always guessing the majority class (positive) would be correct 64.71% of the time |
| McNemar’s Test | p = 3.183e-06 | Suggests a significant difference in types of errors; model is not just guessing randomly |

### Neutral Network: Predicting Ratings with Nuance

Can we predict a more nuanced outcome of the review based on patient attributes, dosage, and drug features?

```{r}
# Drop NAs
drug_train <- drug_train_tsv %>%
  select(rating, urlDrugName, condition, effectiveness) %>%
  drop_na()

# Treat rating as factor
drug_train$rating <- as.factor(drug_train$rating)

drug_train <- drug_train %>%
  mutate(
    urlDrugName = fct_lump(urlDrugName, n = 20),
    condition = fct_lump(condition, n = 20),
    effectiveness = fct_lump(effectiveness, n = 5)
  )

# Split into training and validation sets
set.seed(42)
split_index <- createDataPartition(drug_train$rating, p = 0.8, list = FALSE)
train_set <- drug_train[split_index, ]
valid_set <- drug_train[-split_index, ]

# Train the neural network model
nn_model <- nnet(rating ~ urlDrugName + condition + effectiveness,
                 data = train_set,
                 size = 5,
                 maxit = 200,
                 trace = FALSE)

# Predict on validation set
nn_pred <- predict(nn_model, newdata = valid_set, type = "class")

nn_pred <- factor(nn_pred, levels = levels(valid_set$rating))
valid_rating <- factor(valid_set$rating, levels = levels(valid_set$rating))

# Evaluate model
confusionMatrix(nn_pred, valid_rating)

```

| Class | Sensitivity | Precision | Balanced Accuracy |
|-------|-------------|-----------|-------------------|
| 1     | 50.8%       | 64.6%     | 73.9%             |
| 2     | 0%          | NaN       | 50.0%             |
| 3     | 13.8%       | 12.9%     | 54.6%             |
| 4     | 9.5%        | 50.0%     | 54.6%             |
| 5     | 0%          | 0%        | 49.8%             |
| 6     | 3.2%        | 7.7%      | 50.6%             |
| 7     | 27.1%       | 24.4%     | 58.2%             |
| 8     | 48.6%       | 35.3%     | 64.6%             |
| 9     | 2.1%        | 7.4%      | 48.6%             |
| 10    | 89.9%       | 50.8%     | 81.2%             |

#### **Accuracy**: 0.3981

-   The model correctly predicted the exact rating about 40% of the time.

**No Information Rate (NIR)**: 0.2395

-   Always guessing the most common rating (10) would be correct about 24% of the time.

**P-value (Accuracy \> NIR)**: \< 2.2e-16

-   The model significantly outperforms random guessing or defaulting to the majority class.

**Kappa**: 0.2672

-   This indicates fair agreement beyond chance; falls within 0.2-0.4 range.

### KNN (K-Nearest Neighbors)

Can we classify the sentiment of a new review based on sentiments of similar past reviews?

#### Top 5 Drug Names and Conditions

```{r}
knn_model_columns <- c("rating", "urlDrugName", "condition", "effectiveness")
drug_train_tsv <- drug_train_tsv %>%
  drop_na(any_of(knn_model_columns))
colnames(drug_train_tsv)

top_drugs <- drug_train_tsv %>%
  count(urlDrugName, sort = TRUE) %>%
  slice_head(n = 5) %>%
  pull(urlDrugName)

top_conditions <- drug_train_tsv %>%
  count(condition, sort = TRUE) %>%
  slice_head(n = 5) %>%
  pull(condition)

kable(top_drugs, caption = "Top 5 Drug Names")
kable(top_conditions, caption = "Top 5 Conditions")
```

#### KNN Hyperparameter Tuning & Cross-Validation

```{r, message=FALSE}
library(FNN)
set.seed(123)

#Turns categorial variables into numeric variables for computation
drug_train <- drug_train_tsv %>%
  mutate(
    rating = as.numeric(rating),
    sentiment = ifelse(rating >= 7, "Positive", "Negative")) 

#Gets top drugs and conditions
drug_train <- drug_train %>%
  mutate(
    drug_top = ifelse(urlDrugName %in% top_drugs, urlDrugName, "Other"), 
    condition_top = ifelse(condition %in% top_conditions, condition, "Other") 
  ) %>%
  mutate(
    across(c(drug_top, condition_top), ~ as.factor(.)) # Convert to factors
  )

#Scales and encodes the data
drug_train$rating_scaled <- scale(drug_train$rating)
drug_train_encode <- model.matrix(~ drug_top + condition_top - 1, data = drug_train)
train_predictors <- cbind(drug_train$rating_scaled, drug_train_encode)
train_labels <- factor(drug_train$sentiment)
knn_control <- trainControl(method = "cv", number = 10)

#Cross Validation
knn_cv <- train(
  x = as.data.frame(train_predictors),
  y = train_labels, 
  method = "knn", 
  trControl = knn_control, 
  tuneGrid = expand.grid(k = 1:20) 
  )

plot(knn_cv)
```

#### KNN Classification

```{r}
plot_data_sentiment <- drug_train %>%
  select(rating, ...1, sentiment) %>%
  rename(patientID = ...1) %>% 
  drop_na()

ggplot(data = plot_data_sentiment, aes(x = rating, 
                                       y = patientID, 
                                       color = sentiment)) +
  geom_point(alpha = 0.6) +
  labs(title = "Patient Reviews by Rating and Feature", 
       x = "Patient Rating (1-10)", 
       y = "Patient ID",
       color = "Sentiment") + 
  theme_minimal()
```

### K-means Clustering

```{r, message=FALSE}
library(ggplot2)
library(factoextra)
set.seed(123)

drug_kmeans <- drug_train_tsv %>%
  mutate(rating = as.numeric(rating)) %>%
  drop_na(rating) %>%
  select(rating)

# perform kmeans clustering (k = 5 clusters)
kmeans_res <- kmeans(drug_kmeans, centers = 5, nstart = 20)
drug_KMeanClusters <- kmeans_res$cluster
drug_kmeans$cluster <- as.factor(drug_KMeanClusters)

#Plot k-means clustering
set.seed(123)
plot(drug_kmeans, pch = kmeans_res$cluster, col = kmeans_res$cluster, main = 'k means')
```

```{r}
table(drug_kmeans)
```

## Model Interpretations

### Logistic Regression Model

#### Model Strengths

-   High accuracy (82.9%)
-   Excellent at identifying positive reviews (specificity = 91.8%)
-   High precision for negative class (precision = 81.5%), so false alarms are limited
-   Performs much better than guessing the majority class (accuracy \> NIR)

#### Model Weaknesses

-   Struggles more with detecting negative reviews (sensitivity = 66.5%)
-   Misses about 1 in 3 negative reviews

#### Conclusion

We can predict whether a patient’s review is positive or negative using machine learning. Our logistic regression model, trained on features such as the drug name, medical condition, and reported effectiveness, achieved an overall accuracy of 82.85%, which was much higher than the no-information rate of 64.71%. The model showed strong performance in identifying positive reviews (specificity = 91.8%) and reasonably good performance for negative reviews (sensitivity = 66.5%). These results indicate that the model can reliably classify overall patient sentiment based on the structured data.

The logistic regression model achieved an overall accuracy of 82.85%, which significantly outperforms the no-information rate of 64.71% (p \< 2.2e-16). It performs well in identifying positive reviews (specificity = 91.8%) but is less sensitive to negative reviews (sensitivity = 66.5%). The balanced accuracy of 79.1% confirms that the model maintains reasonable performance across both classes. The confusion matrix and McNemar's test (p = 3.18e-06) further indicate that the model is not biased toward one class and captures meaningful patterns in patient review sentiment.

### Neural Networks Model

#### Model Strengths

-   Performs best on class 10, the most common rating (recall = 89.9%)

-   Moderate sensitivity for Classes 1 and 8, meaning the model is able to detect some negative (1) and mid-range (8) ratings well

#### Model Weaknesses

-   Struggles with middle ratings (2-6), very low recall and precision

-   Leans heavily towards the most frequent classes, possibly due to class imbalance within the dataset

#### Conclusion

The neutral network model achieves moderate overall accuracy (39.8%) and significantly outperforms baseline guessing. However, it shows a strong bias towards frequent ratings like 10, while struggling to accurately detect rare or mid-range ratings (2-6). Future work or revisions should address the class imbalance to improve performance across all review scores.

### K-Nearest Neighbors (KNN) Model

The clusters are based on their centers, the within-cluster variances, and the number of data points that each one contains.

-   **Cluster 1**: 121 + 190 = 311 data points

-   **Cluster 2**: 49 + 44 = 93 data points

-   **Cluster 3**: 115 + 33 = 148 data points

-   **Cluster 4**: 67 + 58 = 125 data points

-   **Cluster 5**: 133 + 226 = 359 data points

**Cluster 5** represents the highest ratings while **Cluster 3** represents the lowest ratings.

#### Model Strengths

The Circle (**Cluster 1**) and Diamond (**Cluster 5**) clusters perform exceptionally well with ratings 7 and over. There is a clear separation between the clusters.

#### Model Weaknesses

The Plus (**Cluster 3**), Triangle (**Cluster 2**), and Cross (**Cluster 4**) clusters perform poorly (less than average) with ratings 6 and below. The model does not guarantee an absolute optimal solution.

#### Conclusion

### K-means Clustering Model

#### Model Strengths

#### Model Weaknesses

#### Conclusion

## Discussion

**Logistic Regression**: excellent at identifying positive reviews, reasonably good at identifying negative reviews

**Neural Networks**: excellent at predicting common scores (ex. 10); struggled with rarer or mid-range ratings

**KNN**: excellent at classification and identifying reviews; concern with “memorizing” instead of “generalizing”

**K-means Clustering**: clusters 1 & 5 performed the best, clear separation of clusters; clusters 2,3,4 performed worse with ratings 6 and below.

## Conclusion

## References

Gräßer, F., Kallumadi, S., Malberg, H., & Zaunseder, S. (2018). Aspect-Based Sentiment Analysis of Drug Reviews Applying Cross-Domain and Cross-Data Learning. Proceedings of the 2018 International Conference on Digital Health.

Hassan, A., Mohasefi, J.B., & Azar, S. (2025). Predicting Sentiments of Users about Medical Treatments using Pre-trained Large Language Models. Journal of Information Systems Engineering and Management.

Kallumadi, S. & Grer, F. (2018). Drug Reviews (Druglib.com) \[Dataset\]. UCI Machine Learning Repository. https://doi.org/10.24432/C55G6J.

## Code Appendix

```{r codeAppend, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
